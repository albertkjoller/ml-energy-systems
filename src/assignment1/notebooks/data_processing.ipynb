{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97932596",
   "metadata": {},
   "source": [
    "# Loading and combining the data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9001e6d-51d5-4f50-8e0f-311c95c232f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40e8f0",
   "metadata": {},
   "source": [
    "#### Define functions for loading and combining separate data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cfb7cbc-16bb-4e13-a8b0-b7cadb83a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find out whether the datetime object of the power production should be associated to StartTimeUTC or EndTimeUTC: \n",
    "### both for the actual power and for the day ahead prices\n",
    "def load_actual_wind_power(DATA_DIR):\n",
    "    print(\"Loading actual wind power production...\")\n",
    "\n",
    "    # Read csv-file\n",
    "    actual_wind_power                   = pd.read_csv(DATA_DIR / 'raw/Actual wind power.csv', sep=';')\n",
    "    # Parse datetime by combining date and hour information\n",
    "    actual_wind_power['StartTimeUTC']   = pd.to_datetime(actual_wind_power['Date'] + ' ' + actual_wind_power['Time'], format='mixed')\n",
    "    # Add timezone to datetime element\n",
    "    actual_wind_power['StartTimeUTC']   = actual_wind_power.StartTimeUTC.dt.tz_localize(pytz.UTC) \n",
    "\n",
    "    # Assume that the timestamp is from DK (since it starts at hour 0 of 2021 which the data from the other files does in DK time.\n",
    "    # For this reason we adjust the timeseries and express everything in terms of UTC timestamps \n",
    "    actual_wind_power['StartTimeUTC']   = actual_wind_power.StartTimeUTC - pd.to_timedelta('2 hours')\n",
    "\n",
    "    # Get rid of redundant columns\n",
    "    actual_wind_power = actual_wind_power.drop(columns=['Date', 'Time'])\n",
    "    return actual_wind_power\n",
    "\n",
    "def load_balancing_prices(DATA_DIR):\n",
    "    print(\"Loading balancing prices...\")\n",
    "\n",
    "    # Load information about up- and down-regulation prices for both 2021 and 2022\n",
    "    for year_idx, year in enumerate([2021, 2022]):\n",
    "        for i, filename in enumerate([f'Down-regulation price_{year}.csv', f'Up-regulation price_{year}.csv']):\n",
    "            # Determine filetype\n",
    "            price_type          = filename.split('-')[0]\n",
    "            price_column_name   = 'Up-regulating' if price_type == 'Up' else 'Down-regulation' \n",
    "            price_column_name   = f'\"{price_column_name} price in the Balancing energy market\"\"\"'\n",
    "            \n",
    "            # Read csv-file as temporary dataframe\n",
    "            df_price_ = pd.read_csv(DATA_DIR / f'raw/{filename}', sep=',\"', engine='python')\n",
    "        \n",
    "            # Handle encoding with quotation marks\n",
    "            df_price_['StartTimeUTC']   = pd.to_datetime(df_price_['\"Start time UTC'].str.strip('\"')).dt.tz_localize(pytz.UTC)\n",
    "            df_price_['EndTimeUTC']     = pd.to_datetime(df_price_['\"End time UTC\"\"'].str.strip('\"')).dt.tz_localize(pytz.UTC)\n",
    "\n",
    "            # Change datatype of prices from string to float\n",
    "            df_price_[f'BalancingMarketPrice_{price_type}Reg'] = df_price_[price_column_name].str.strip('\"')\n",
    "            df_price_[f'BalancingMarketPrice_{price_type}Reg'] = df_price_[f'BalancingMarketPrice_{price_type}Reg'].astype(float)\n",
    "        \n",
    "            # Restrict data to relevant information - Danish timezone is implicitly contained in UTC timestamp\n",
    "            df_price_ = df_price_[['StartTimeUTC', 'EndTimeUTC', f'BalancingMarketPrice_{price_type}Reg']]\n",
    "\n",
    "\n",
    "            # Combine dataframes for both years \n",
    "            prices_ = df_price_ if i == 0 else prices_.merge(df_price_, on=['StartTimeUTC', 'EndTimeUTC'], how='outer')\n",
    "            \n",
    "        # Merge prices from year with currently stored price information into combined dataframe\n",
    "        balancing_prices = prices_ if year_idx == 0 else pd.concat([balancing_prices, prices_], axis=0).reset_index(drop=True)\n",
    "        \n",
    "    return balancing_prices\n",
    "\n",
    "# TODO: find out whether the datetime object of the power production should be associated to StartTimeUTC or EndTimeUTC: \n",
    "### both for the actual power and for the day ahead prices\n",
    "def load_day_ahead_prices(DATA_DIR):\n",
    "    print(\"Loading day-ahead prices...\")\n",
    "\n",
    "    # Read day ahead prices from excel sheet\n",
    "    day_ahead_prices = pd.read_excel(DATA_DIR / 'raw/Day-ahead price.xlsx')\n",
    "\n",
    "    # Represent time as datetime object\n",
    "    day_ahead_prices['StartTimeUTC'] = pd.to_datetime(day_ahead_prices['HourUTC']).dt.tz_localize(pytz.UTC)\n",
    "\n",
    "    # Get rid of redundant information\n",
    "    day_ahead_prices = day_ahead_prices[['StartTimeUTC', 'PriceArea', 'SpotPriceDKK', 'SpotPriceEUR']]\n",
    "    return day_ahead_prices\n",
    "\n",
    "\n",
    "def combine_and_save_data_sources(DATA_DIR: Path, SAVE_DIR: Path):\n",
    "    # Load and do initial processing of data files\n",
    "    actual_wind_power   = load_actual_wind_power(DATA_DIR)\n",
    "    balancing_prices    = load_balancing_prices(DATA_DIR)\n",
    "    day_ahead_prices    = load_day_ahead_prices(DATA_DIR)\n",
    "\n",
    "    # Merge data sources based on temporal information and pricearea\n",
    "    dataset = actual_wind_power.merge(day_ahead_prices, on='StartTimeUTC', how='left').merge(balancing_prices, on='StartTimeUTC')\n",
    "\n",
    "    # TODO: consider what to do with summer/wintertime hours - here we take the mean\n",
    "    dataset = dataset.groupby(by=['StartTimeUTC', 'EndTimeUTC', 'PriceArea']).mean().reset_index()\n",
    "\n",
    "    # Save loaded and combined data files\n",
    "    dataset.to_csv(SAVE_DIR / 'combined.csv')\n",
    "    actual_wind_power.to_csv(SAVE_DIR / 'actual_wind_power.csv')\n",
    "    balancing_prices.to_csv(SAVE_DIR / 'balancing_prices.csv')\n",
    "    day_ahead_prices.to_csv(SAVE_DIR / 'day_ahead_prices.csv')\n",
    "    \n",
    "    return dataset, actual_wind_power, balancing_prices, day_ahead_prices    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aec578",
   "metadata": {},
   "source": [
    "#### Load and save data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a743cc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading actual wind power production...\n",
      "Loading balancing prices...\n",
      "Loading day-ahead prices...\n"
     ]
    }
   ],
   "source": [
    "# Set path to data and save folder\n",
    "DATA_DIR = Path('../../../data/assignment1')\n",
    "SAVE_DIR = Path('../../../data/assignment1/processed')\n",
    "\n",
    "# Load and combine data sources\n",
    "dataset, _, _, _ = combine_and_save_data_sources(DATA_DIR, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32042d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values occuring in the dataset? False\n"
     ]
    }
   ],
   "source": [
    "print(f\"NaN values occuring in the dataset? {dataset.isna().any().any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded06417",
   "metadata": {},
   "source": [
    "#### Exploration and loading of weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d058543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import zipfile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce03b90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique weather attributes: \n",
      "['no_ice_days' 'temp_grass' 'leaf_moisture' 'mean_temp' 'mean_wind_speed'\n",
      " 'max_temp_w_date' 'mean_cloud_cover' 'temp_soil_30' 'no_summer_days'\n",
      " 'temp_soil_10' 'mean_daily_max_temp' 'no_lightning_strikes'\n",
      " 'max_wind_speed_10min' 'bright_sunshine' 'no_tropical_nights'\n",
      " 'no_cold_days' 'no_days_acc_precip_1' 'min_temp' 'drought_index'\n",
      " 'mean_radiation' 'no_days_acc_precip_01' 'acc_heating_degree_days_17'\n",
      " 'max_wind_speed_3sec' 'vapour_pressure_deficit_mean'\n",
      " 'pot_evaporation_makkink' 'mean_pressure' 'mean_daily_min_temp'\n",
      " 'mean_relative_hum' 'acc_precip' 'mean_wind_dir' 'no_days_acc_precip_10'\n",
      " 'no_frost_days' 'max_precip_30m' 'snow_depth']\n",
      "\n",
      "Unique municipalities: \n",
      "['Furesø' 'Struer' 'Egedal' 'Faxe' 'Fredericia' 'Frederikssund' 'Hvidovre'\n",
      " 'Lemvig' 'Kalundborg' 'Vesthimmerlands' 'Lejre' 'Haderslev' 'Syddjurs'\n",
      " 'Thisted' 'Allerød' 'Odense' 'Solrød' 'Svendborg' 'Hørsholm'\n",
      " 'Lyngby-Taarbæk' 'Gribskov' 'Ikast-Brande' 'Vordingborg' 'Stevns' 'Samsø'\n",
      " 'Sønderborg' 'Roskilde' 'Guldborgsund' 'Varde' 'Ringsted' 'Tårnby'\n",
      " 'Odder' 'Hedensted' 'Randers' 'Frederikshavn' 'Slagelse' 'Rebild'\n",
      " 'Nordfyns' 'Vejen' 'Fredensborg' 'Vallensbæk' 'Herlev' 'Dragør' 'Ishøj'\n",
      " 'Vejle' 'Skive' 'Ballerup' 'Nyborg' 'Glostrup' 'Esbjerg' 'Albertslund'\n",
      " 'Bornholm' 'Tønder' 'Billund' 'Sorø' 'Morsø' 'Silkeborg' 'Ærø' 'Køge'\n",
      " 'Aarhus' 'Horsens' 'Faaborg-Midtfyn' 'Gladsaxe' 'Halsnæs' 'Favrskov'\n",
      " 'Kerteminde' 'Gentofte' 'Hillerød' 'Lolland' 'Høje-Taastrup'\n",
      " 'Skanderborg' 'Assens' 'Viborg' 'Brønderslev' 'Rødovre' 'København'\n",
      " 'Hjørring' 'Norddjurs' 'Langeland' 'Herning' 'Næstved' 'Holstebro'\n",
      " 'Middelfart' 'Odsherred' 'Jammerbugt' 'Mariagerfjord' 'Kolding'\n",
      " 'Helsingør' 'Brøndby' 'Rudersdal' 'Frederiksberg' 'Ringkøbing-Skjern'\n",
      " 'Fanø' 'Aalborg' 'Læsø' 'Aabenraa' 'Holbæk' 'Greve']\n"
     ]
    }
   ],
   "source": [
    "# zip file handler  \n",
    "zip = zipfile.ZipFile('C:/Users/alber/Desktop/DTU/3_HCAI/46765/ml-energy-systems/data/assignment1/raw/Climate data_2021.zip')\n",
    "f = zip.open('2021-01-01.txt', 'r')\n",
    "\n",
    "# Do initial investigation of relevant attributes by loading a single sample file\n",
    "information_list = []\n",
    "for line in f:\n",
    "    information_list.append(json.loads(line))\n",
    "\n",
    "# Ex\n",
    "weather_attributes = pd.Series([information['properties']['parameterId'] for information in information_list]).unique()\n",
    "print(f\"Unique weather attributes: \\n{weather_attributes}\")\n",
    "\n",
    "municipalities = pd.Series([information['properties']['municipalityName'] for information in information_list]).unique()\n",
    "print(f\"\\nUnique municipalities: \\n{municipalities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4342f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_climate_data(year, DATA_DIR, SAVE_DIR):\n",
    "    \n",
    "    # Open zip folder\n",
    "    zip = zipfile.ZipFile(DATA_DIR / f'raw/Climate data_{year}.zip')\n",
    "\n",
    "    # Extract information from all files within the zip-folder\n",
    "    weather_information = []\n",
    "    for i, filename in enumerate(tqdm(zip.namelist(), desc=f'Processing {year}')):\n",
    "        # Open file\n",
    "        f = zip.open(filename, 'r')\n",
    "        for line in f:\n",
    "            # Read each entry individually\n",
    "            information = json.loads(line)\n",
    "            \n",
    "            # Restrict the extracted information to temporal, location and weather-related information only \n",
    "            new_observation = [\n",
    "                information['properties']['from'], \n",
    "                information['properties']['to'], \n",
    "                information['properties']['parameterId'], \n",
    "                information['properties']['value'], \n",
    "                information['geometry']['coordinates'][0], \n",
    "                information['geometry']['coordinates'][1], \n",
    "                information['properties']['municipalityName']\n",
    "            ]\n",
    "            weather_information.append(new_observation)\n",
    "\n",
    "    # Create dataframe of weather from the given year\n",
    "    column_names        = ['StartTimeUTC', 'EndTimeUTC', 'WeatherAttribute', 'Value', 'Longitude', 'Latitude', 'Municipality'] \n",
    "    weather_information = pd.DataFrame(weather_information, columns=column_names)\n",
    "    \n",
    "    # Save information\n",
    "    weather_information.to_csv(SAVE_DIR / f'weather_information_{year}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "138bcec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2021: 100%|██████████| 365/365 [05:39<00:00,  1.07it/s]\n",
      "Processing 2022: 100%|██████████| 365/365 [02:44<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set path to data and save folder\n",
    "DATA_DIR = Path('../../../data/assignment1')\n",
    "SAVE_DIR = Path('../../../data/assignment1/processed')\n",
    "\n",
    "process_climate_data(2021, DATA_DIR, SAVE_DIR)\n",
    "process_climate_data(2022, DATA_DIR, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf92b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4bd34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec892f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
