{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97932596",
   "metadata": {},
   "source": [
    "# Loading and combining the data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9001e6d-51d5-4f50-8e0f-311c95c232f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import os\n",
    "\n",
    "import pytz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import json\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40e8f0",
   "metadata": {},
   "source": [
    "#### Define functions for loading and combining separate data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5849cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "\n",
    "    def __init__(self, DATA_DIR: Path, SAVE_DIR: Path):\n",
    "        self.DATA_DIR = DATA_DIR\n",
    "        self.SAVE_DIR = SAVE_DIR\n",
    "\n",
    "    # TODO: find out whether the datetime object of the power production should be associated to StartTimeUTC or EndTimeUTC: \n",
    "    ### both for the actual power and for the day ahead prices\n",
    "    def load_actual_wind_power(self):\n",
    "        print(\"Loading actual wind power production...\")\n",
    "\n",
    "        # Read csv-file\n",
    "        actual_wind_power                   = pd.read_csv(self.DATA_DIR / 'raw/Actual wind power.csv', sep=';')\n",
    "        # Parse datetime by combining date and hour information\n",
    "        actual_wind_power['StartTimeUTC']   = pd.to_datetime(actual_wind_power['Date'] + ' ' + actual_wind_power['Time'], format='mixed')\n",
    "        # Add timezone to datetime element\n",
    "        actual_wind_power['StartTimeUTC']   = actual_wind_power.StartTimeUTC.dt.tz_localize(pytz.UTC) \n",
    "\n",
    "        # Assume that the timestamp is from DK (since it starts at hour 0 of 2021 which the data from the other files does in DK time.\n",
    "        # For this reason we adjust the timeseries and express everything in terms of UTC timestamps \n",
    "        actual_wind_power['StartTimeUTC']   = actual_wind_power.StartTimeUTC - pd.to_timedelta('2 hours')\n",
    "\n",
    "        # Get rid of redundant columns\n",
    "        actual_wind_power = actual_wind_power.drop(columns=['Date', 'Time'])\n",
    "        return actual_wind_power\n",
    "\n",
    "    def load_balancing_prices(self):\n",
    "        print(\"Loading balancing prices...\")\n",
    "\n",
    "        # Load information about up- and down-regulation prices for both 2021 and 2022\n",
    "        for year_idx, year in enumerate([2021, 2022]):\n",
    "            for i, filename in enumerate([f'Down-regulation price_{year}.csv', f'Up-regulation price_{year}.csv']):\n",
    "                # Determine filetype\n",
    "                price_type          = filename.split('-')[0]\n",
    "                price_column_name   = 'Up-regulating' if price_type == 'Up' else 'Down-regulation' \n",
    "                price_column_name   = f'\"{price_column_name} price in the Balancing energy market\"\"\"'\n",
    "                \n",
    "                # Read csv-file as temporary dataframe\n",
    "                df_price_ = pd.read_csv(self.DATA_DIR / f'raw/{filename}', sep=',\"', engine='python')\n",
    "            \n",
    "                # Handle encoding with quotation marks\n",
    "                df_price_['StartTimeUTC']   = pd.to_datetime(df_price_['\"Start time UTC'].str.strip('\"')).dt.tz_localize(pytz.UTC)\n",
    "                df_price_['EndTimeUTC']     = pd.to_datetime(df_price_['\"End time UTC\"\"'].str.strip('\"')).dt.tz_localize(pytz.UTC)\n",
    "\n",
    "                # Change datatype of prices from string to float\n",
    "                df_price_[f'BalancingMarketPrice_{price_type}Reg'] = df_price_[price_column_name].str.strip('\"')\n",
    "                df_price_[f'BalancingMarketPrice_{price_type}Reg'] = df_price_[f'BalancingMarketPrice_{price_type}Reg'].astype(float)\n",
    "            \n",
    "                # Restrict data to relevant information - Danish timezone is implicitly contained in UTC timestamp\n",
    "                df_price_ = df_price_[['StartTimeUTC', 'EndTimeUTC', f'BalancingMarketPrice_{price_type}Reg']]\n",
    "\n",
    "\n",
    "                # Combine dataframes for both years \n",
    "                prices_ = df_price_ if i == 0 else prices_.merge(df_price_, on=['StartTimeUTC', 'EndTimeUTC'], how='outer')\n",
    "                \n",
    "            # Merge prices from year with currently stored price information into combined dataframe\n",
    "            balancing_prices = prices_ if year_idx == 0 else pd.concat([balancing_prices, prices_], axis=0).reset_index(drop=True)\n",
    "            \n",
    "        return balancing_prices\n",
    "\n",
    "    # TODO: find out whether the datetime object of the power production should be associated to StartTimeUTC or EndTimeUTC: \n",
    "    ### both for the actual power and for the day ahead prices\n",
    "    def load_day_ahead_prices(self):\n",
    "        print(\"Loading day-ahead prices...\")\n",
    "\n",
    "        # Read day ahead prices from excel sheet\n",
    "        day_ahead_prices = pd.read_excel(self.DATA_DIR / 'raw/Day-ahead price.xlsx')\n",
    "\n",
    "        # Represent time as datetime object\n",
    "        day_ahead_prices['StartTimeUTC'] = pd.to_datetime(day_ahead_prices['HourUTC']).dt.tz_localize(pytz.UTC)\n",
    "\n",
    "        # Get rid of redundant information\n",
    "        day_ahead_prices = day_ahead_prices[['StartTimeUTC', 'PriceArea', 'SpotPriceDKK', 'SpotPriceEUR']]\n",
    "        return day_ahead_prices\n",
    "\n",
    "    def preprocess_climate_data(self, year):\n",
    "        ### Preprocess the raw climate data and saves restructured csv file that can then be restricted to e.g. Roskilde municipality\n",
    "        # Open zip folder\n",
    "        zip = zipfile.ZipFile(self.DATA_DIR / f'raw/Climate data_{year}.zip')\n",
    "\n",
    "        # Extract information from all files within the zip-folder\n",
    "        weather_information = []\n",
    "        for i, filename in enumerate(tqdm(zip.namelist(), desc=f'Processing {year}')):\n",
    "            # Open file\n",
    "            f = zip.open(filename, 'r')\n",
    "            for line in f:\n",
    "                # Read each entry individually\n",
    "                information = json.loads(line)\n",
    "                \n",
    "                # Restrict the extracted information to temporal, location and weather-related information only \n",
    "                new_observation = [\n",
    "                    information['properties']['from'], \n",
    "                    information['properties']['to'], \n",
    "                    information['properties']['parameterId'], \n",
    "                    information['properties']['value'], \n",
    "                    information['geometry']['coordinates'][0], \n",
    "                    information['geometry']['coordinates'][1], \n",
    "                    information['properties']['municipalityName']\n",
    "                ]\n",
    "                weather_information.append(new_observation)\n",
    "\n",
    "        # Create dataframe of weather from the given year\n",
    "        column_names        = ['StartTimeUTC', 'EndTimeUTC', 'WeatherAttribute', 'Value', 'Longitude', 'Latitude', 'Municipality'] \n",
    "        weather_information = pd.DataFrame(weather_information, columns=column_names)\n",
    "        \n",
    "        # Save information\n",
    "        weather_information.to_csv(self.SAVE_DIR / f'weather_information_{year}.csv')  \n",
    "\n",
    "    def load_weather_from_municipality(self, municipality: str):\n",
    "        print(f\"Loading weather data from {municipality}...\")\n",
    "        # If preprocessing has not been run, do it!\n",
    "        for year in [2021, 2022]:\n",
    "            if not os.path.isfile(self.SAVE_DIR / f'weather_information_{year}.csv'):\n",
    "                self.process_climate_data(year) # preprocessing function\n",
    "\n",
    "        # Load the saved weather information and average it\n",
    "        weather = pd.DataFrame()\n",
    "        for year in [2021, 2022]:\n",
    "            print(f\"\\t --> Loading {year}...\")\n",
    "            # Load \"preprocessed\" DMI data\n",
    "            weather_ = pd.read_csv(self.SAVE_DIR / f'weather_information_{year}.csv', index_col=0)\n",
    "            \n",
    "            # Restrict weather information to wind-related attributes\n",
    "            weather_ = weather_.query(\n",
    "                'WeatherAttribute == \"mean_wind_speed\" or WeatherAttribute == \"mean_wind_dir\" or WeatherAttribute == \"max_wind_speed_10min\" or WeatherAttribute == \"max_wind_speed_3sec\"'\n",
    "            )\n",
    "\n",
    "            # Restrict weather measures to location of Roskilde (as this is where the actual power production) is from\n",
    "            weather_ = weather_.query(f'Municipality == \"{municipality}\"')\n",
    "\n",
    "            # Merge weather information from both years\n",
    "            weather = pd.concat([weather, weather_], axis=0).reset_index(drop=True)\n",
    "\n",
    "        # Map timestrings as timestamps\n",
    "        weather['StartTimeUTC'] = pd.to_datetime(weather['StartTimeUTC'].progress_apply(lambda x: parse(x)), utc=True)\n",
    "        weather['EndTimeUTC']   = pd.to_datetime(weather['EndTimeUTC'].progress_apply(lambda x: parse(x)), utc=True)\n",
    "\n",
    "        # Remove hours where there for some reason are \n",
    "        weather     = weather[weather['StartTimeUTC'].dt.microsecond == 0].sort_values('StartTimeUTC').reset_index(drop=True)\n",
    "        avg_weather = weather.groupby(by=['StartTimeUTC', 'EndTimeUTC', 'WeatherAttribute'])['Value'].mean().reset_index()\n",
    "        avg_weather = avg_weather.pivot(index=['StartTimeUTC', 'EndTimeUTC'], columns=['WeatherAttribute'], values='Value').reset_index()\n",
    "        return avg_weather\n",
    "    \n",
    "    def combine_and_save_data_sources(self, weather_municipality: str = 'Roskilde', priceareas: List[str] = ['DK2']):\n",
    "        \n",
    "        # Load and do initial processing of data files\n",
    "        actual_wind_power   = self.load_actual_wind_power()\n",
    "        balancing_prices    = self.load_balancing_prices()\n",
    "        day_ahead_prices    = self.load_day_ahead_prices()\n",
    "        avg_weather         = self.load_weather_from_municipality(weather_municipality)\n",
    "\n",
    "        # Merge data sources based on temporal information and pricearea\n",
    "        dataset = actual_wind_power.merge(day_ahead_prices, on='StartTimeUTC', how='left')\n",
    "        dataset = dataset.merge(balancing_prices, on='StartTimeUTC')\n",
    "\n",
    "        # TODO: consider what to do with summer/wintertime hours - here we take the mean\n",
    "        dataset = dataset.groupby(by=['StartTimeUTC', 'EndTimeUTC', 'PriceArea']).mean().reset_index()\n",
    "        \n",
    "        # Add weather information to merged dataset\n",
    "        dataset = dataset.merge(avg_weather, on=['StartTimeUTC', 'EndTimeUTC'])\n",
    "\n",
    "        # Save loaded and combined data files\n",
    "        for pricearea in priceareas:\n",
    "            dataset.query(f'PriceArea == \"{pricearea}\"').to_csv(self.SAVE_DIR / f'{pricearea}.csv')\n",
    "\n",
    "        dataset.to_csv(self.SAVE_DIR / f'AllPriceAreas.csv')\n",
    "        actual_wind_power.to_csv(self.SAVE_DIR / 'actual_wind_power.csv')\n",
    "        balancing_prices.to_csv(self.SAVE_DIR / 'balancing_prices.csv')\n",
    "        day_ahead_prices.to_csv(self.SAVE_DIR / 'day_ahead_prices.csv')\n",
    "        avg_weather.to_csv(self.SAVE_DIR / f'avg_weather_{weather_municipality}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded06417",
   "metadata": {},
   "source": [
    "#### Exploration and loading of weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce03b90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique weather attributes: \n",
      "['no_ice_days' 'temp_grass' 'leaf_moisture' 'mean_temp' 'mean_wind_speed'\n",
      " 'max_temp_w_date' 'mean_cloud_cover' 'temp_soil_30' 'no_summer_days'\n",
      " 'temp_soil_10' 'mean_daily_max_temp' 'no_lightning_strikes'\n",
      " 'max_wind_speed_10min' 'bright_sunshine' 'no_tropical_nights'\n",
      " 'no_cold_days' 'no_days_acc_precip_1' 'min_temp' 'drought_index'\n",
      " 'mean_radiation' 'no_days_acc_precip_01' 'acc_heating_degree_days_17'\n",
      " 'max_wind_speed_3sec' 'vapour_pressure_deficit_mean'\n",
      " 'pot_evaporation_makkink' 'mean_pressure' 'mean_daily_min_temp'\n",
      " 'mean_relative_hum' 'acc_precip' 'mean_wind_dir' 'no_days_acc_precip_10'\n",
      " 'no_frost_days' 'max_precip_30m' 'snow_depth']\n",
      "\n",
      "Unique municipalities: \n",
      "['Furesø' 'Struer' 'Egedal' 'Faxe' 'Fredericia' 'Frederikssund' 'Hvidovre'\n",
      " 'Lemvig' 'Kalundborg' 'Vesthimmerlands' 'Lejre' 'Haderslev' 'Syddjurs'\n",
      " 'Thisted' 'Allerød' 'Odense' 'Solrød' 'Svendborg' 'Hørsholm'\n",
      " 'Lyngby-Taarbæk' 'Gribskov' 'Ikast-Brande' 'Vordingborg' 'Stevns' 'Samsø'\n",
      " 'Sønderborg' 'Roskilde' 'Guldborgsund' 'Varde' 'Ringsted' 'Tårnby'\n",
      " 'Odder' 'Hedensted' 'Randers' 'Frederikshavn' 'Slagelse' 'Rebild'\n",
      " 'Nordfyns' 'Vejen' 'Fredensborg' 'Vallensbæk' 'Herlev' 'Dragør' 'Ishøj'\n",
      " 'Vejle' 'Skive' 'Ballerup' 'Nyborg' 'Glostrup' 'Esbjerg' 'Albertslund'\n",
      " 'Bornholm' 'Tønder' 'Billund' 'Sorø' 'Morsø' 'Silkeborg' 'Ærø' 'Køge'\n",
      " 'Aarhus' 'Horsens' 'Faaborg-Midtfyn' 'Gladsaxe' 'Halsnæs' 'Favrskov'\n",
      " 'Kerteminde' 'Gentofte' 'Hillerød' 'Lolland' 'Høje-Taastrup'\n",
      " 'Skanderborg' 'Assens' 'Viborg' 'Brønderslev' 'Rødovre' 'København'\n",
      " 'Hjørring' 'Norddjurs' 'Langeland' 'Herning' 'Næstved' 'Holstebro'\n",
      " 'Middelfart' 'Odsherred' 'Jammerbugt' 'Mariagerfjord' 'Kolding'\n",
      " 'Helsingør' 'Brøndby' 'Rudersdal' 'Frederiksberg' 'Ringkøbing-Skjern'\n",
      " 'Fanø' 'Aalborg' 'Læsø' 'Aabenraa' 'Holbæk' 'Greve']\n"
     ]
    }
   ],
   "source": [
    "# zip file handler  \n",
    "zip = zipfile.ZipFile('C:/Users/alber/Desktop/DTU/3_HCAI/46765/ml-energy-systems/data/assignment1/raw/Climate data_2021.zip')\n",
    "f = zip.open('2021-01-01.txt', 'r')\n",
    "\n",
    "# Do initial investigation of relevant attributes by loading a single sample file\n",
    "information_list = []\n",
    "for line in f:\n",
    "    information_list.append(json.loads(line))\n",
    "\n",
    "# Ex\n",
    "weather_attributes = pd.Series([information['properties']['parameterId'] for information in information_list]).unique()\n",
    "print(f\"Unique weather attributes: \\n{weather_attributes}\")\n",
    "\n",
    "municipalities = pd.Series([information['properties']['municipalityName'] for information in information_list]).unique()\n",
    "print(f\"\\nUnique municipalities: \\n{municipalities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f84f51b",
   "metadata": {},
   "source": [
    "#### Load and save data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56130561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to data and save folder\n",
    "DATA_DIR = Path('../../../data/assignment1')\n",
    "SAVE_DIR = Path(r'../../../data/assignment1/processed')\n",
    "\n",
    "# Define dataset processor object\n",
    "processor          = DataProcessor(DATA_DIR=DATA_DIR, SAVE_DIR=SAVE_DIR)\n",
    "\n",
    "# Load and combine data sources\n",
    "processor.combine_and_save_data_sources(weather_municipality='Roskilde', priceareas=['DK1', 'DK2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69af2996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values occuring in DK1 dataset? False\n",
      "NaN values occuring in DK2 dataset? False\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = Path(r'../../../data/assignment1/processed')\n",
    "\n",
    "# Load DK1 and DK2 datasets\n",
    "DK1_dataset = pd.read_csv(SAVE_DIR / 'DK1.csv', index_col=0)\n",
    "DK2_dataset = pd.read_csv(SAVE_DIR / 'DK2.csv', index_col=0)\n",
    "\n",
    "# Check if missing data occurs\n",
    "print(f\"NaN values occuring in DK1 dataset? {DK1_dataset.isna().any().any()}\")\n",
    "print(f\"NaN values occuring in DK2 dataset? {DK2_dataset.isna().any().any()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
